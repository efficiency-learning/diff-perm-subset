{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d71fb9f-94d5-4e51-b469-587c14f657eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22115952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0,0,0,0], dtype=torch.float32)\n",
    "torch.nn.functional.normalize(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1934c77d-28ce-4fea-ba2b-13decf173234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc9efa1-dcd7-45d2-9aca-3afdac9b4679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54760118 0.80149694 0.24119498 0.89909611]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.54760118, 0.24119498, 0.89909611, 0.80149694])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.random.rand(4)\n",
    "print(xx)\n",
    "xx[[0,2,3,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f9f7c-88dc-45c2-9b7f-a7e43dbf171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "    auction_lap.py\n",
    "    \n",
    "    From\n",
    "        https://dspace.mit.edu/bitstream/handle/1721.1/3265/P-2108-26912652.pdf;sequence=1\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def auction_lap(X, eps=None, compute_score=True):\n",
    "    \"\"\"\n",
    "        X: n-by-n matrix w/ integer entries\n",
    "        eps: \"bid size\" -- smaller values means higher accuracy w/ longer runtime\n",
    "    \"\"\"\n",
    "    eps = 1 / X.shape[0] if eps is None else eps\n",
    "    \n",
    "    # --\n",
    "    # Init\n",
    "    \n",
    "    cost     = torch.zeros((1, X.shape[1]))\n",
    "    curr_ass = torch.zeros(X.shape[0]).long() - 1\n",
    "    bids     = torch.zeros(X.shape)\n",
    "    \n",
    "    if X.is_cuda:\n",
    "        cost, curr_ass, bids = cost.cuda(), curr_ass.cuda(), bids.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    while (curr_ass == -1).any():\n",
    "        counter += 1\n",
    "        \n",
    "        # --\n",
    "        # Bidding\n",
    "        \n",
    "        unassigned = (curr_ass == -1).nonzero().squeeze(dim=1)\n",
    "        \n",
    "        value = X[unassigned] - cost\n",
    "        top_value, top_idx = value.topk(2, dim=1)\n",
    "        \n",
    "        first_idx = top_idx[:,0]\n",
    "        first_value, second_value = top_value[:,0], top_value[:,1]\n",
    "        \n",
    "        bid_increments = first_value - second_value + eps\n",
    "        \n",
    "        bids_ = bids[unassigned]\n",
    "        bids_.zero_()\n",
    "        bids_.scatter_(\n",
    "            dim=1,\n",
    "            index=first_idx.contiguous().view(-1, 1),\n",
    "            src=bid_increments.view(-1, 1)\n",
    "        )\n",
    "        \n",
    "        # --\n",
    "        # Assignment\n",
    "        \n",
    "        have_bidder = (bids_ > 0).int().sum(dim=0).nonzero()\n",
    "        \n",
    "        high_bids, high_bidders = bids_[:,have_bidder].max(dim=0)\n",
    "        high_bidders = unassigned[high_bidders.squeeze()]\n",
    "        \n",
    "        cost[:,have_bidder] += high_bids\n",
    "        \n",
    "        curr_ass[(curr_ass.view(-1, 1) == have_bidder.view(1, -1)).sum(dim=1)] = -1\n",
    "        curr_ass[high_bidders] = have_bidder.squeeze()\n",
    "    \n",
    "    score = None\n",
    "    if compute_score:\n",
    "        score = int(X.gather(dim=1, index=curr_ass.view(-1, 1)).sum())\n",
    "    \n",
    "    return score, curr_ass, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daffaaad-e671-4cad-a396-e75ebce29e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0103,  0.2311, -0.0290,  0.2136,  0.6728],\n",
      "         [-0.6718,  1.1757,  0.3734, -0.4217, -0.9985],\n",
      "         [ 0.5374, -1.0040, -0.1930,  1.1668,  1.2807],\n",
      "         [-1.3198,  0.5633,  0.1314,  0.9520,  0.5832],\n",
      "         [ 0.1003,  1.2608,  0.1722, -2.2411,  1.4001]],\n",
      "\n",
      "        [[ 1.2192,  2.2065, -1.6999,  0.0936,  0.6548],\n",
      "         [ 1.2671, -0.3542, -0.9693, -0.4955, -0.5529],\n",
      "         [-0.3040,  1.8883,  0.7900,  0.0718, -0.3346],\n",
      "         [-0.5126, -1.6058,  0.3922, -0.4387, -0.2765],\n",
      "         [ 0.4620, -0.1736,  1.4381, -1.1730,  0.2142]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 3, 2, 4],\n",
       "        [4, 0, 1, 3, 2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_linear_assignment import batch_linear_assignment\n",
    "\n",
    "xx = torch.randn(2,5,5)\n",
    "print(xx)\n",
    "batch_linear_assignment(-xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "083db67e-b690-46be-be36-ce9e65a39d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 3, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "linear_sum_assignment(-xx.numpy()[1,:])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1176fa0d-decc-4573-943d-3eea07c28475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f074327-b189-45e7-83c2-de05a6204f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(torch.ones(2,2), [1, torch.ones(2,2).shape[0], torch.ones(2,2).shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1397ded-bbc5-48f3-94da-73f404b43765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860e283-6668-4941-8a33-a5137e8189ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.6, 0.8], [0.3,0.7]\n",
    "0.75, 0.25\n",
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d26ddbb4-3684-4837-9190-9032efa5b118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc4ae5b-8871-4d29-90c9-7c8d145fd293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f506819-e4ac-4d96-9d27-3dd6e94ccf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "def sinkhorn_forward(C, mu, nu, epsilon, max_iter):\n",
    "    bs, n, k_ = C.size()\n",
    "\n",
    "    v = torch.ones([bs, 1, k_])/(k_)\n",
    "    G = torch.exp(-C/epsilon)\n",
    "    if torch.cuda.is_available():\n",
    "        v = v.cuda()\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        u = mu/(G*v).sum(-1, keepdim=True)\n",
    "        v = nu/(G*u).sum(-2, keepdim=True)\n",
    "\n",
    "    Gamma = u*G*v\n",
    "    return Gamma\n",
    "\n",
    "def sinkhorn_forward_stablized(C, mu, nu, epsilon, max_iter):\n",
    "    bs, n, k_ = C.size()\n",
    "    k = k_-1\n",
    "\n",
    "    f = torch.zeros([bs, n, 1])\n",
    "    g = torch.zeros([bs, 1, k+1])\n",
    "    if torch.cuda.is_available():\n",
    "        f = f.cuda()\n",
    "        g = g.cuda()\n",
    "\n",
    "    epsilon_log_mu = epsilon*torch.log(mu)\n",
    "    epsilon_log_nu = epsilon*torch.log(nu)\n",
    "\n",
    "    def min_epsilon_row(Z, epsilon):\n",
    "        return -epsilon*torch.logsumexp((-Z)/epsilon, -1, keepdim=True)\n",
    "    \n",
    "    def min_epsilon_col(Z, epsilon):\n",
    "        return -epsilon*torch.logsumexp((-Z)/epsilon, -2, keepdim=True)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        f = min_epsilon_row(C-g, epsilon)+epsilon_log_mu\n",
    "        g = min_epsilon_col(C-f, epsilon)+epsilon_log_nu\n",
    "        \n",
    "    Gamma = torch.exp((-C+f+g)/epsilon)\n",
    "    return Gamma\n",
    "    \n",
    "def sinkhorn_backward(grad_output_Gamma, Gamma, mu, nu, epsilon):\n",
    "    \n",
    "    nu_ = nu[:,:,:-1]\n",
    "    Gamma_ = Gamma[:,:,:-1]\n",
    "\n",
    "    bs, n, k_ = Gamma.size()\n",
    "    \n",
    "    inv_mu = 1./(mu.view([1,-1]))  #[1, n]\n",
    "    Kappa = torch.diag_embed(nu_.squeeze(-2)) \\\n",
    "            -torch.matmul(Gamma_.transpose(-1, -2) * inv_mu.unsqueeze(-2), Gamma_)   #[bs, k, k]\n",
    "    \n",
    "    inv_Kappa = torch.inverse(Kappa) #[bs, k, k]\n",
    "    \n",
    "    Gamma_mu = inv_mu.unsqueeze(-1)*Gamma_\n",
    "    L = Gamma_mu.matmul(inv_Kappa) #[bs, n, k]\n",
    "    G1 = grad_output_Gamma * Gamma #[bs, n, k+1]\n",
    "    \n",
    "    g1 = G1.sum(-1)\n",
    "    G21 = (g1*inv_mu).unsqueeze(-1)*Gamma  #[bs, n, k+1]\n",
    "    g1_L = g1.unsqueeze(-2).matmul(L)  #[bs, 1, k]\n",
    "    G22 = g1_L.matmul(Gamma_mu.transpose(-1,-2)).transpose(-1,-2)*Gamma  #[bs, n, k+1]\n",
    "    G23 = - F.pad(g1_L, pad=(0, 1), mode='constant', value=0)*Gamma  #[bs, n, k+1]\n",
    "    G2 = G21 + G22 + G23  #[bs, n, k+1]\n",
    "    \n",
    "    del g1, G21, G22, G23, Gamma_mu\n",
    "    \n",
    "    g2 = G1.sum(-2).unsqueeze(-1) #[bs, k+1, 1]\n",
    "    g2 = g2[:,:-1,:]  #[bs, k, 1]\n",
    "    G31 = - L.matmul(g2)*Gamma  #[bs, n, k+1]\n",
    "    G32 = F.pad(inv_Kappa.matmul(g2).transpose(-1,-2), pad=(0, 1), mode='constant', value=0)*Gamma  #[bs, n, k+1]\n",
    "    G3 = G31 + G32  #[bs, n, k+1]\n",
    "\n",
    "    grad_C = (-G1+G2+G3)/epsilon  #[bs, n, k+1]\n",
    "    return grad_C\n",
    "\n",
    "class TopKFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, C, mu, nu, epsilon, max_iter):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if epsilon>1e-2:\n",
    "                Gamma = sinkhorn_forward(C, mu, nu, epsilon, max_iter)\n",
    "                if bool(torch.any(Gamma!=Gamma)):\n",
    "                    print('Nan appeared in Gamma, re-computing...')\n",
    "                    Gamma = sinkhorn_forward_stablized(C, mu, nu, epsilon, max_iter)\n",
    "            else:\n",
    "                Gamma = sinkhorn_forward_stablized(C, mu, nu, epsilon, max_iter)\n",
    "            ctx.save_for_backward(mu, nu, Gamma)\n",
    "            ctx.epsilon = epsilon\n",
    "        return Gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_Gamma):\n",
    "        \n",
    "        epsilon = ctx.epsilon\n",
    "        mu, nu, Gamma = ctx.saved_tensors\n",
    "        # mu [1, n, 1]\n",
    "        # nu [1, 1, k+1]\n",
    "        #Gamma [bs, n, k+1]   \n",
    "        with torch.no_grad():\n",
    "            grad_C = sinkhorn_backward(grad_output_Gamma, Gamma, mu, nu, epsilon)\n",
    "        return grad_C, None, None, None, None\n",
    "\n",
    "\n",
    "class TopK_custom(torch.nn.Module):\n",
    "    def __init__(self, k, epsilon=0.1, max_iter = 200):\n",
    "        super(TopK_custom, self).__init__()\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.anchors = torch.FloatTensor([k-i for i in range(k+1)]).view([1,1, k+1])\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.anchors = self.anchors.cuda()\n",
    "\n",
    "    def forward(self, scores):\n",
    "        bs, n = scores.size()\n",
    "        scores = scores.view([bs, n, 1])\n",
    "        \n",
    "        #find the -inf value and replace it with the minimum value except -inf\n",
    "        scores_ = scores.clone().detach()\n",
    "        max_scores = torch.max(scores_).detach()\n",
    "        scores_[scores_==float('-inf')] = float('inf')\n",
    "        min_scores = torch.min(scores_).detach()\n",
    "        filled_value = min_scores - (max_scores-min_scores)\n",
    "        mask = scores==float('-inf')\n",
    "        scores = scores.masked_fill(mask, filled_value)\n",
    "        \n",
    "        C = (scores-self.anchors)**2\n",
    "        C = C / (C.max().detach())\n",
    "      \n",
    "        mu = torch.ones([1, n, 1], requires_grad=False)/n\n",
    "        nu = [1./n for _ in range(self.k)]\n",
    "        nu.append((n-self.k)/n)\n",
    "        nu = torch.FloatTensor(nu).view([1, 1, self.k+1])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            mu = mu.cuda()\n",
    "            nu = nu.cuda()\n",
    "            \n",
    "        Gamma = TopKFunc.apply(C, mu, nu, self.epsilon, self.max_iter)\n",
    " \n",
    "        A = Gamma[:,:,:self.k]*n\n",
    "        \n",
    "        return A, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75d455e-bb9e-4e08-b10a-961ba0024093",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = TopK_custom(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "527109a6-5501-4f84-861f-527d850a33c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0041, 0.0059, 0.0084, 0.0117, 0.0161, 0.0218, 0.0291, 0.0380,\n",
       "           0.0488, 0.0613],\n",
       "          [0.0058, 0.0080, 0.0110, 0.0147, 0.0195, 0.0255, 0.0327, 0.0411,\n",
       "           0.0508, 0.0615],\n",
       "          [0.0340, 0.0380, 0.0420, 0.0457, 0.0491, 0.0520, 0.0540, 0.0552,\n",
       "           0.0552, 0.0542],\n",
       "          [0.0246, 0.0287, 0.0331, 0.0376, 0.0421, 0.0464, 0.0502, 0.0534,\n",
       "           0.0558, 0.0570],\n",
       "          [0.1205, 0.1105, 0.1001, 0.0894, 0.0787, 0.0683, 0.0582, 0.0487,\n",
       "           0.0400, 0.0322],\n",
       "          [0.0172, 0.0210, 0.0253, 0.0300, 0.0351, 0.0404, 0.0457, 0.0508,\n",
       "           0.0555, 0.0593],\n",
       "          [0.0043, 0.0062, 0.0087, 0.0121, 0.0165, 0.0223, 0.0295, 0.0384,\n",
       "           0.0491, 0.0614],\n",
       "          [0.0311, 0.0352, 0.0394, 0.0434, 0.0471, 0.0504, 0.0530, 0.0548,\n",
       "           0.0555, 0.0551],\n",
       "          [0.0390, 0.0429, 0.0465, 0.0497, 0.0524, 0.0544, 0.0555, 0.0556,\n",
       "           0.0547, 0.0527],\n",
       "          [0.0294, 0.0335, 0.0377, 0.0419, 0.0459, 0.0494, 0.0524, 0.0545,\n",
       "           0.0556, 0.0556],\n",
       "          [0.0894, 0.0866, 0.0829, 0.0782, 0.0728, 0.0666, 0.0600, 0.0531,\n",
       "           0.0460, 0.0391],\n",
       "          [0.1650, 0.1414, 0.1196, 0.0998, 0.0821, 0.0665, 0.0530, 0.0414,\n",
       "           0.0318, 0.0239],\n",
       "          [0.0665, 0.0676, 0.0679, 0.0672, 0.0656, 0.0631, 0.0596, 0.0553,\n",
       "           0.0503, 0.0449],\n",
       "          [0.0601, 0.0620, 0.0632, 0.0636, 0.0631, 0.0616, 0.0591, 0.0557,\n",
       "           0.0515, 0.0466],\n",
       "          [0.0030, 0.0044, 0.0065, 0.0093, 0.0133, 0.0187, 0.0259, 0.0351,\n",
       "           0.0467, 0.0608],\n",
       "          [0.0576, 0.0598, 0.0614, 0.0622, 0.0620, 0.0609, 0.0588, 0.0558,\n",
       "           0.0519, 0.0473],\n",
       "          [0.0338, 0.0379, 0.0419, 0.0456, 0.0491, 0.0519, 0.0540, 0.0551,\n",
       "           0.0553, 0.0542],\n",
       "          [0.0240, 0.0280, 0.0324, 0.0369, 0.0415, 0.0459, 0.0499, 0.0533,\n",
       "           0.0558, 0.0573],\n",
       "          [0.1049, 0.0988, 0.0919, 0.0843, 0.0762, 0.0678, 0.0594, 0.0510,\n",
       "           0.0430, 0.0355],\n",
       "          [0.0855, 0.0835, 0.0805, 0.0765, 0.0717, 0.0662, 0.0601, 0.0535,\n",
       "           0.0468, 0.0401]]], device='cuda:0'),\n",
       " None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top(torch.randn(1,20).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5315bfb-95f8-40da-aa95-975c95d7238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  2.04s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:15<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def _find_ts(xs, ks, binary_iter=16, newton_iter=1):\n",
    "    n = xs.shape[-1]\n",
    "    assert torch.all((0 < ks) & (ks < n)), \"We don't support k=0 or k=n\"\n",
    "    # Lo should be small enough that all sigmoids are in the 0 area.\n",
    "    # Similarly Hi is large enough that all are in their 1 area.\n",
    "    lo = -xs.max(dim=-1, keepdims=True).values - 10\n",
    "    hi = -xs.min(dim=-1, keepdims=True).values + 10\n",
    "    assert torch.all(torch.sigmoid(xs + lo).sum(dim=-1) < 1)\n",
    "    assert torch.all(torch.sigmoid(xs + hi).sum(dim=-1) > n - 1)\n",
    "    # Batch binary search, solving sigmoid(xs + ts) = ks\n",
    "    for _ in range(binary_iter):\n",
    "        mid = (hi + lo) / 2\n",
    "        mask = torch.sigmoid(xs + mid).sum(dim=-1) < ks\n",
    "        lo[mask] = mid[mask]\n",
    "        hi[~mask] = mid[~mask]\n",
    "    ts = (lo + hi) / 2\n",
    "    # Fine-tune using some Newton iterations\n",
    "    for _ in range(newton_iter):\n",
    "        sig = torch.sigmoid(xs + ts)\n",
    "        den = sig.sum(dim=-1, keepdims=True) - ks[..., None]\n",
    "        num = (sig * (1 - sig)).sum(dim=-1, keepdims=True)\n",
    "        ts -= den / num\n",
    "    # Test for success\n",
    "    assert torch.allclose(torch.sigmoid(xs + ts).sum(dim=-1), ks.double())\n",
    "    return ts\n",
    "\n",
    "\n",
    "class TopK(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, xs, ks):\n",
    "        ts = _find_ts(xs, ks)\n",
    "        ps = torch.sigmoid(xs + ts)\n",
    "        ctx.save_for_backward(ps)\n",
    "        return ps\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Compute vjp, that is grad_output.T @ J.\n",
    "        (ps,) = ctx.saved_tensors\n",
    "        # Let v = sigmoid'(x + t)\n",
    "        v = ps * (1 - ps)  # sigmoid' = sigmoid * (1 - sigmoid)\n",
    "        s = v.sum(dim=-1, keepdims=True)\n",
    "        t_d = v / s\n",
    "        # Jacobian is -vv.T/s + diag(v)\n",
    "        uv = grad_output * v\n",
    "        t1 = uv.sum(dim=-1, keepdims=True) * t_d\n",
    "        return uv - t1, None\n",
    "\n",
    "\n",
    "class TopK_BCE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, xs, ks, ys):\n",
    "        xs = xs + _find_ts(xs, ks)\n",
    "        ctx.save_for_backward(xs, ks, ys)\n",
    "        loss = (ys - 1) * xs + F.logsigmoid(xs)\n",
    "        return -loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        xts, ks, ys = ctx.saved_tensors\n",
    "        # Compute d/dxi t = - sig'(x_i + t) / sum_j sig'(x_j + t)\n",
    "        sig = torch.sigmoid(xts)\n",
    "        sig_d = sig * (1 - sig)  # sigmoid' = sigmoid * (1 - sigmoid)\n",
    "        num = sig_d.sum(dim=-1, keepdims=True)\n",
    "        t_d = -sig_d / num\n",
    "        # Jacobian is t'e^T - diag(e)\n",
    "        e = ys - sig\n",
    "        ev = e * grad_output\n",
    "        b = ev + t_d * ev.sum(dim=-1, keepdims=True)\n",
    "        return -b, None, xts\n",
    "\n",
    "\n",
    "soft_topk = TopK.apply\n",
    "bce_topk = TopK_BCE.apply\n",
    "\n",
    "\n",
    "def main():\n",
    "    from torch.autograd import gradcheck\n",
    "    import tqdm\n",
    "\n",
    "    n1, n2, d = 20, 2, 10\n",
    "    xs = torch.randn(n1, n2, d, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "    # Test TopK function\n",
    "    for _ in tqdm.tqdm(range(2)):\n",
    "        ks = torch.randint(1, d, size=(n1, n2))\n",
    "        assert gradcheck(soft_topk, (xs, ks), eps=1e-6, atol=1e-4)\n",
    "\n",
    "    for _ in tqdm.tqdm(range(10)):\n",
    "        ks = torch.randint(1, d, size=(n1, n2), dtype=torch.double)\n",
    "        ys = torch.randint(0, 2, size=(n1, n2, d), dtype=torch.double)\n",
    "        # Test forward method\n",
    "        torch.testing.assert_close(\n",
    "            F.binary_cross_entropy(soft_topk(xs, ks), ys, reduction=\"none\"),\n",
    "            bce_topk(xs, ks, ys),\n",
    "        )\n",
    "        # Test backward method\n",
    "        assert gradcheck(bce_topk, (xs, ks, ys), eps=1e-6, atol=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef7f0aa-9880-474a-8770-2280abf47a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1, n2, d = 20, 2, 10\n",
    "print(torch.randint(1, d, size=(n1, n2)).shape)\n",
    "torch.randn(n1, n2, d, dtype=torch.double, requires_grad=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb509dc4-bd12-499a-b75b-cf07f5680e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_linear_assignment import batch_linear_assignment\n",
    "\n",
    "\n",
    "def my_matching(matrix_batch):\n",
    "    \"\"\"\n",
    "    Solves a matching problem for a batch of matrices using the Straight-Through Estimator (STE).\n",
    "\n",
    "    Args:\n",
    "        matrix_batch: A 3D tensor (a batch of matrices) with shape = [batch_size, N, N].\n",
    "                      If 2D, the input is reshaped to 3D with batch_size = 1.\n",
    "\n",
    "    Returns:\n",
    "        A 2D integer tensor of permutations with shape [batch_size, N].\n",
    "    \"\"\"\n",
    "    \n",
    "    def hungarian(x):\n",
    "        if x.ndim == 2:\n",
    "            x = torch.reshape(x, [1, x.shape[0], x.shape[1]])\n",
    "        sol = batch_linear_assignment(-x)\n",
    "        return sol\n",
    "\n",
    "    N = matrix_batch.shape[-1]\n",
    "    index_layer = torch.nn.Linear(N, 1, bias=False)\n",
    "        \n",
    "    # Initialize the weights to the range of vocab_size and freeze the layer\n",
    "    with torch.no_grad():\n",
    "        index_layer.weight = torch.nn.Parameter(torch.arange(N).float().unsqueeze(0))\n",
    "    index_layer.weight.requires_grad = False\n",
    "\n",
    "    # Get hard permutations using Hungarian algorithm\n",
    "    listperms_hard = hungarian(matrix_batch.detach()).to(matrix_batch.device)  # Detach to prevent gradient tracking\n",
    "\n",
    "    batch_size, N, _ = matrix_batch.shape\n",
    "    listperms_hard_onehot = torch.zeros(batch_size, N, N, device=matrix_batch.device)\n",
    "    listperms_hard_onehot.scatter_(2, listperms_hard.unsqueeze(-1), 1)\n",
    "\n",
    "    # STE: During backward, replace the gradient with the gradient of matrix_batch\n",
    "    listperms_ste = listperms_hard_onehot + matrix_batch - matrix_batch.detach()\n",
    "\n",
    "    # Convert back to hard permutations (indices) as the output\n",
    "    #listperms_output = listperms_hard_onehot.argmax(dim=-1).float()\n",
    "    \n",
    "    return listperms_ste, index_layer(listperms_ste).squeeze(-1)\n",
    "    #return listperms_ste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91526169-1159-4738-8b8d-941101f69efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000]]], grad_fn=<SubBackward0>), tensor([[4.0000, 1.0000, 3.0000, 2.0000, 0.0000],\n",
      "        [2.0000, 4.0000, 3.0000, 1.0000, 0.0000],\n",
      "        [4.0000, 2.0000, 0.0000, 1.0000, 3.0000],\n",
      "        [4.0000, 1.0000, 3.0000, 0.0000, 2.0000],\n",
      "        [3.0000, 0.0000, 4.0000, 2.0000, 1.0000],\n",
      "        [4.0000, 3.0000, 0.0000, 2.0000, 1.0000],\n",
      "        [3.0000, 4.0000, 0.0000, 1.0000, 2.0000],\n",
      "        [4.0000, 3.0000, 0.0000, 2.0000, 1.0000],\n",
      "        [2.0000, 0.0000, 1.0000, 4.0000, 3.0000],\n",
      "        [0.0000, 4.0000, 1.0000, 3.0000, 2.0000]], grad_fn=<SqueezeBackward1>))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(listperms)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Example loss and backward pass\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(listperms)  \u001b[38;5;66;03m# Define some loss function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "batch_size, N = 10, 5\n",
    "matrix_batch = torch.rand(batch_size, N, N, requires_grad=True)\n",
    "listperms = my_matching(matrix_batch)\n",
    "print(listperms)\n",
    "# Example loss and backward pass\n",
    "loss = torch.sum(listperms)  # Define some loss function\n",
    "loss.backward()  # Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddf9449e-90d6-4216-94eb-57f12b649a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5)[[4.0000, 1.0000, 3.0000, 2.0000, 0.0000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147f81f-a679-4815-a100-ca6f864693c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scsnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
